import numpy as np
import xarray as xr
import pandas as pd
import scipy.io as sio
import datetime, warnings
import os.path
from abc import ABC, abstractmethod


''' 
Module for loading, preparing, merging, and saving data from geophysical 
simulations and experiments. 
Testing has focused on physical oceanography including gridded datasets 
from both in-situ and remote observations as well as model output. 
Most data is managed as xarray objects, except for some instruments that
are best represented by pandas dataframes. 

Classes in this module: 
    matfile
        datenum_to_datetime    
    assemble 
        retreive
        store_nc
        remove_limit_row
        concat_xrs
        interp_grid
    saver (it has two modes. to netcdf and mat)

Rogue functions:
    clean_chivar
    mat2xr

What are some metaclasses and better structures that could streamline data loading
and increase versatility to work with diverse datasets? ////

- Written by Noel G. Brizuela
- Scripps Institution of Oceanography
- nogutier@ucsd.edu
- January of 2022
'''

class saver(ABC):
    def __init__(self, object_list):
        self.obj_list = object_list; # may include many object types
        self.settings = dict(); 

    def set_destination(self, filename):
        self.settings['filename'] = os.path.basename(filename); 
        self.settings['directory'] = os.path.abspath(filename);

    def gather_xrs(self):
        ''' Run through xrs in self.obj_list, identify instances of xr.DataArray
        and xr.Dataset, and prepare those to be saved. 
        For simplicity, it will be assumed that all dataarrays can be merged into
        a single dataset.
        '''
        for obj in self.obj_list:
            pass
        if isinstance(obj, xr.DataArray):
                pass

    
class to_netcdf(saver):
    def __init__(self, object_list):
        pass

class to_matfile(saver):
    def __init__(self, object_list):
        pass

def clean_chivar(data_dict, flags, var2clean, as_xr=False):
    # chi_data is a dict produced by reading multiple variables in matfile
    # flags is a pd.dataframe extracted from csv
    # var2clean is a list of strings identifying data
    # Use flag file to average only valid estimates in data
    
    SN = data_dict['SN'][0];
    Ninst = len(data_dict['SN'][0])
    Nmeas = len(data_dict['time']);
    varmat = np.empty( (Nmeas, Ninst)); varmat[:]=np.nan; 
    
    for inst in range(Ninst):
        estsel = flags.loc[inst][1:]; # flags for instrument
        all_data = data_dict[var2clean][inst,0];
        #var_here = data_dict[var2clean][inst,0][estsel,:];
        for zind in range(len(estsel.values)):
            if estsel[zind] == False:
                all_data[zind,:] = np.nan;
        with warnings.catch_warnings():
            warnings.simplefilter("ignore",category=RuntimeWarning);
            varmat[:,inst] = np.nanmean( all_data, axis=0);

    # Now save in a dataframe
    if as_xr:
        chidat = xr.DataArray(data=varmat, dims=['time','SN'],
                    coords={'time':data_dict['time'],'SN':['SN'+str(kk) for kk in SN]});	
    else: 
        chidat = pd.DataFrame( data=varmat, index=data_dict['time'],
                              columns='SN'+str(SN)); 
        chidat.index = pd.to_datetime(chidat.index);
        
    return chidat

# Loader functions and datatype classes will be defined in this module. 
class assemble:
    def __init__(self, obj_id, paths ):
        self.name = obj_id['name'];
        self.type = obj_id['type'];
        self.project = obj_id['project']; 
        self.limits = obj_id['limits'];
        self.paths = paths; 
        
        # Now list properties that will be generated by class methods.
        self.grids = dict(); # they will be pandas, numpy, and xarray objects.
        self.vars = dict(); 
     
    def retrieve(self,filename, fileformat, **kwargs):
        if fileformat in ['.nc','.netcdf','.nc4']:
            dataset = xr.open_dataset(filename, **kwargs);
        elif fileformat == '.mat':
            dataset = sio.loadmat(filename, **kwargs);
        return dataset
            
    def store_nc(self,filepaths,varname,limits='none',saveas='default', **kwargs): 
		# mat support is experimental
        # This method will:
        # 1. Load dataset from "filename"
        # 2. Extract variable grid from "varname" in that dataset. 
        # 3. Store list of variable's grid inside of self.grids
        cutgrid = isinstance(limits,dict);  
        jj=0;   
        for item in varname: 
            myvar = []; # store vals here
            for kk in range(len(filepaths)): 
                filename = filepaths[kk]; 
                datform = os.path.splitext(filename); # get data format
                dataset = self.retrieve(filename,datform[1], **kwargs);

                myvar.insert(kk, dataset[item]);   
            #   If cut=True, load only a slice into memory 
                if cutgrid:
                    myvar.insert(kk, myvar[kk].sel(time=slice(limits['t0'],limits['t1'])) );         

        # Store list of grids in dictionary
            if saveas=='default':
                self.grids[item] = myvar;
            else:
                self.grids[saveas[jj]] = myvar;
            jj += 1;
        
    
    def cut_grids(self,gridnames,limits):
        # Gridnames is a list of objects that correspond to keys in self.grids
        # Let us access each grid in that list and make a subselection of data with xr.sel
        #for kk in range(len(gridnames)):
        #    this_var = self.grids[gridnames[kk]]
        #    for item in this_var:
        #        for dimensh in limits:
        pass        


    def remove_limit_row(self, variable, item, dirax, howmany, first=False):
        # Remove the first or last row of non-nan measurements along a dimension
        check_limit = ~np.isnan( self.grids[variable][int(item)].values);
        if first:
            check_limit = np.cumsum( check_limit, axis=dirax);
        else:
            check_limit = np.sum(check_limit,axis=dirax) - np.cumsum(check_limit,axis=dirax);
        ax2run = abs(dirax-1); 
        grid_length = self.grids[variable][item].shape[ax2run]; 
        new_var = self.grids[variable][item].values; 
        for tt in range(grid_length):
            check_here = check_limit.take(indices=tt,axis=ax2run);
            where_z = check_here<howmany;
            #print(where_z)
            where_t = np.repeat(tt,len(where_z));
            #print(where_t)
            new_var[where_z, tt] = np.nan;
        self.grids[variable][item] = xr.DataArray( data=new_var, 
                    dims=self.grids[variable][item].dims, coords= \
                    self.grids[variable][item].coords); 
    def makeplot(self):
        self.grids['u'][2].plot()
        
    def concat_xrs(self, variable, sorter, sort=True, wiggle=False):
        ''' Go to self.grids[variable] and concatenate all xr objects stored in that list.
        Concatenation will occur along the sorter (string) dimension 
        '''
        datlist = self.grids[variable] # list of objects for this variable
        vardata = []; varcoords = []; # store data and coordinats separately
        all_inst = xr.concat( datlist, dim=sorter ); # sorter is a string
        if wiggle:
            all_inst[sorter] = all_inst[sorter] + 1e-5*np.random.rand( len(all_inst[sorter]));
        if sort:
            all_inst = all_inst.sortby(sorter); 
        return all_inst
        
        
    def interp_grid(self, variables, sorter , wiggle=False):
        '''  All grids in self.grids[variables[kk]] are interpolated onto 
        a new set of axes sorter is the str() for whatever dim unites 
        measurements for moorings it's usually depth or z '''
        source_coords = dict(); 
        source_data = dict(); 
        cake = [];     
        # Cyle through all variables requested
        for kk in range(len(variables)):
            all_inst = self.concat_xrs(variables[kk], sorter, sort=True, wiggle=True); 
            if all_inst.dims[1] == sorter: # THIS IS VERY SPECIFIC TO MOORING APPLICATION
                all_inst = all_inst.transpose(); # only works for 2d objects

            all_inst = all_inst.interpolate_na(dim=sorter); # INTERPOLATION WON'T BE NECESSARY IN MANY CASES
            self.vars[variables[kk]] = all_inst; # replace list for single xr object
     
#    def make_multivar(self, vars2get, filepaths):
		# Input a dictionary whose keys are variable names, and content are 
		# lists with files where each variable is stored 
	
#    def unifygrid(self, variable, common_vecs):
#        varexists = variable in self.grids; 
#        if varexists == False:
#            print('Variable files have not been loaded');
#            return 
        
    def gridtime(self, varname):
        gridobj = self.grids[varname];
        return gridobj.time

''' BELOW ARE THE FUNCTIONS THAT ALLOW .MAT SUPPORT '''
class matfile:
    def __init__(self,filepath, notes='none'):
        self.path = filepath; 
        self.notes= notes;  
        self.data = sio.loadmat(filepath); 
	
    def which_keys(self):
        print(self.data.keys())

    def read_var(self, variable):
        if isinstance(variable,str):
            var_data = self.data[variable]; 
        elif isinstance(variable,list): 
            var_data = dict(); 
            for item in variable:
                var_data[item] = self.data[item]; 
        return var_data;
        
    def read_struct(self, structname, vars2get): 
        struct_data = dict(); 
        for item in vars2get:
            struct_data[item] = self.data[structname][0][item][0];
        return struct_data
        
    def read_cell(self, cellname, indices2get):
        matcell = self.data[cellname][0,0]; # extract only usable info
        list_form = [matcell[index][0] for index in indices2get];
        return list_form;     

    def datenum_to_datetime(self, dnumvec): 
    # Take in a sequence of matlab datenum objects and transform  
    # into python datetimes 
        ref_date = datetime.datetime.strptime('Jan 01 1900 00:00','%b %d %Y %H:%M');
        ref_num = 693962; # ref_date in matlab format
        py_deltas = [datetime.timedelta(days=(dtn-ref_num)) for dtn in dnumvec]; 
        py_dates = [ref_date + dt for dt in py_deltas];
        return py_dates 
    
    def var_to_xr(self, varnames, dimnames, in_struct='none',datenumdim='none'):
        # Assynes tgat ciirdubate are stored under the names of dimnames
        if in_struct=='none':
            my_coords = self.read_var(dimnames);
            var_data = self.read_var(varnames);	
        else:
            my_coords = self.read_struct( in_struct, dimnames);
            var_data = self.read_struct( in_struct, varnames ); 
            for dim in dimnames: 
                if len(my_coords[dim].shape) == 2:
                    my_coords[dim] = np.reshape( my_coords[dim],-1); 
                if dim == datenumdim:
                    my_coords[dim] = self.datenum_to_datetime(my_coords[dim]); 
        
        xrobj = xr.DataArray(data=var_data[varnames[0]], coords=my_coords, dims=dimnames); 	
        return xrobj
        
        
        
def mat2xr(matfile,varnames,dimnames,struct_name='none',to_grid=False):
	# This assumes that coordinates are stored under the same dimname
	# inside the matfile.
	# struct_name refers to whether variables and coords are inside a mat structure

    if struct_name=='none':
        coords = readmat( matfile, dimnames); 
        data = readmat( matfile, varnames); # creates dictionary
    else:
        my_data = readmat( matfile, [], {struct_name:varnames}); 
        my_coords = readmat( matfile, [], {struct_name:dimnames}); 
        # Remove one dimension from coordinate data
        for dim in dimnames:
            if len(my_coords[dim].shape) == 2:
                my_coords[dim] = np.reshape( my_coords[dim],-1);
            if dim == 'time': # change date format 
                my_coords[dim] = datenum(my_coords[dim]);  
    xrobj = xr.DataArray(data=my_data[varnames[0]], coords=my_coords, dims=dimnames);  
    return xrobj
